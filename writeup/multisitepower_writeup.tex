\documentclass[]{article}

% math packages
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}

% for coloring in a table
%\usepackage[table,xcdraw]{xcolor}

% including graphics
\usepackage{graphicx}
\graphicspath{ {./images/} }

% drawing graphs
\usepackage{tikz-cd}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!10]
\tikzstyle{arrow} = [thick,->,>=stealth]

% hyperlinks
\usepackage{hyperref}

% some useful shortcuts
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\indep}{\perp\!\!\!\!\perp}
\newcommand{\blambda}{{\bm{\lambda}}}
\newcommand{\btheta}{{\bm{\theta}}}
\newcommand{\bpsi}{{\bm{\psi}}}

\newcommand{\by}{\mathbf{y}}

\usepackage{setspace}
\doublespacing

% Editing macros
\usepackage{color}
\newcommand\cmnt[2]{\qquad{{\color{red} \em #1---#2} \qquad}}
\newcommand\cmntM[1]{\cmnt{#1}{Miratrix}}
\newcommand\cmntC[1]{\cmnt{#1}{Che}}
\newcommand\awk{{{\color{red} {$\leftarrow$ Awkward phrasing}}\qquad}}
\newcommand\cmntMp[1]{{\color{red} $\leftarrow$ {\em #1 -Miratrix} \qquad}}



%opening
\title{Power calculations for detecting \\ individual site impacts}
\author{Jonathan Che \& Luke Miratrix}

\begin{document}

\maketitle

%\begin{abstract}
%\end{abstract}


\section{Introduction}

The usual question for multisite trials\textemdash randomized trials where each of a set of sites has individuals randomized into treatment and control\textemdash is whether the treatment worked \emph{on average overall}, even though the treatment may have different effects across sites.\footnote{Though seemingly simple, even this question has its wrinkles if we believe the site-level average effects differ.
For example, do we estimate the average impact for all individuals across all sites, or instead estimate the simple site average of their average impacts?}
When designing a multisite experiment there are power analysis tools designed to ensure a given design will achieve desired levels of power for this average effect.
There are even power formulas designed to ensure one can detect a given level of cross-site impact variation, if that is a quantity of interest.

But what if we are interested in the individual sites?
Typically, in a multisite experiment no individual site will be large enough to be well-powered, on its own, for detecting whether treatment at that site was actually effective.
This is, after all, why we frequently turn to multisite experiments: we seek to increase our overall power by averaging across a collection of underpowered, local, investigations.
That being said, the stakeholders at these local investigations will frequently want to know not whether the experiment worked overall, but whether it worked for them.
As researchers, we might also want to identify which sites are most likely to be the drivers of an overall effect.

We can get an approximate answer to these questions about individual sites by using multilevel or hierarchical Bayesian models to partially pool the individual site effects.
For each site, these models ``borrow strength'' from the other sites under the assumption that the sites are related.
This process results in point estimates for each of the individual sites that are shrunken towards the overall average estimated effect.

The construction of confidence intervals around these shrunken estimates requires some nuance.
In general, confidence intervals for individual site effects in a hierarchical model are ill-defined under a frequentist framework.\footnote{Under a frequentist framework, the individual site effects $\tau_j$ are random effects, and not parameters of interest.
Just as it makes little sense to define confidence intervals for regression residuals, it makes little sense to define confidence intervals for these random effects.
(To be honest, I'm not positive about this intuition.
Section 1.2 of  \href{https://core.ac.uk/download/pdf/38903061.pdf}{this paper} describes frequentist inference for random effects, though it mentions that it's quite messy.
On the other hand, the internet (e.g., \href{https://stat.ethz.ch/pipermail/r-sig-mixed-models/2011q3/014077.html}{here} or \href{https://github.com/lme4/lme4/issues/497}{here}) suggests that people don't really do frequentist inference on the random effects.) }
We can, however, construct credible intervals under a Bayesian framework that can provide similar information about the uncertainty in site effects.
There is an extensive literature concerning the construction of appropriate intervals for individual site effects under shrinkage (see Casella and Hwang (2012) for a thorough review; see He (1992), Hwang et al. (2009), and Armstrong et al. (2021) for relevant examples of these methods).

Unfortunately, these credible intervals do not satisfy the frequentist definition of $\alpha$-level converage for the $\tau_j$ values; instead, they satisfy so-called Empirical Bayes (EB) coverage, which integrates the coverage probability over both the data and the parameters (Morris 1983).\footnote{Recall that frequentist coverage is defined as the probability that a random interval contains a fixed parameter $\theta$, where randomness is integrated over the distribution of the observed data.
	EB coverage additionally integrates out the parameter $\theta$ over its prior distribution.}
This means that they only provide guarantees for \textit{average} coverage across sites, and not for coverage for any particular site.
These guarantees are not sufficient for stakeholders in multisite trials who are specifically interested in knowing whether the experiment worked for their site.
For example, the inherent bias in shrunken point estimates raises concerns that only examining average coverage could mask systematic undercoverage for sites with extreme effects, which would be a problem for stakeholders at such sites.
Overall, the question of inference for particular site effects in multilevel models appears to remain fairly open (Armstrong et al (2021) cites Hansen (2006) for this), with limited guidance about best practices.

In this study, we focus on frequentist-style conditional power analyses for detecting a hypothesized effect for a specific site in a particular study design.\footnote{I.e., we are interested in power/coverage, conditioned on given $\tau_j$ values, and not in average power/coverage across the collection of $\tau_j$ values in a multisite trial.}
While there is an extensive literature on these types of power analyses for the overall average treatment effect and cross-site variation in multisite trials (e.g., Raudenbush \& Liu 2000, many others...), less attention has been paid to similar power analyses for individual site effect estimates.
We first review how one might use multilevel and Bayesian models to report such effects.
We then propose simulation-based tools to do power calculations.
Finally, we conduct a simulation study to examine power for site-level treatment effects in multisite trials.
At this point, we also examine how poorly these site-level estimates perform in practice, and provide guidance on the overall business of individual site-effect estimation in the context of multisite trials.


\section{Detecting individual site effects}

Assume a model for individuals $i$ in sites $j$ of: 
\begin{align*}
	Y_{ij} &= \alpha_j + \tau_j Z_{ij} + \epsilon_{ij} \\
	\alpha_j &= \alpha + u_{0j} \\
	\tau_j &= \tau + u_{1j} \\
	\begin{pmatrix}
		u_{0j} \\ u_{1j}
	\end{pmatrix} &\sim N\left(
	\begin{pmatrix}
		0 \\ 0
	\end{pmatrix}, 
	\begin{bmatrix}
		\rho_{00} & \rho_{01} \\ \rho_{10} & \rho_{11}
	\end{bmatrix}\right) \\
	\epsilon_{ij} &\sim N(0, \sigma^2) ,
\end{align*}
where $Z_{ij}$ are the individual treatment indicators, $\tau$ is the overall site-average treatment impact, and the $\rho$ parameters control the covariance of the random effects.
The $u_{0j}$ are random intercepts for control outcomes across sites (with variance $\rho_{00}$) and the $u_{1j}$ are random treatment effects across sites (with variance $\rho_{11}$).

We simulate data under this model, using some simplifying assumptions.
We set $\rho{10} = \rho_{01} = 0$ so that intercept and treatment random effects are uncorrelated.
We also fix $Var(Y_{ij}(0)) = Var(\alpha_j) + Var(\epsilon_{ij}) = 1$ so that the control-unit outcomes are in effect size units.
This scaling gives the variance of our random intercepts in terms of the Intra Class Correlation Coefficient (ICC) of:
$$ICC = \frac{Var(\alpha_i)}{Var(\alpha_j) + Var(\epsilon_{ij})} = Var(\alpha_j) = \rho_{00}$$
This also dictates that the within-site residual variation must be $\sigma^2 = 1-ICC$ so that the control-unit outcomes have variance 1 within each site.
We ignore additional covariate adjustments for the moment for clarity.

The simplest, completely unpooled test of whether an individual site had a positive effect would be to drop the rest of the data and test the difference in means of the treatment and control groups at that site.
The power to detect effects locally in this manner is purely a function of the site sample size, variation in outcomes within the site, and the size of the true site average impact.
The standard error for site $j$'s impact estimate would then be
$$ SE_j = \left[ \frac{1}{n_j} \frac{1}{(1-p_j)p_j} (1-ICC) \right]^{1/2} , $$ 
where $p_j$ is the proportion treated at site $j$.
For testing, we would estimate $\hat{\tau}_j$ and $\widehat{SE}_j$, the latter with a plug-in estimate of $\hat{\sigma}_j$.\footnote{For two examples, we could estimate $\hat{\sigma}_j$ with an interacted linear regression where we completely pool our residual variation across sites, or simply estimate the within-group variation at site $j$.
The former would help stabilize, under homoscedasticity, our residual estimate which could be helpful in very small sample contexts.}
We would finally obtain $p$-values using $t = \hat{\tau}_j / \widehat{SE}_j$, referenced to a $t$ distribution with many degrees of freedom, assuming the site is not tiny.

A multisite analysis pools data across sites to try to improve effect estimates.
With a multilevel model, we first estimate the overall parameters, including the degree of cross site variation $\omega$, and none of the individual site average impacts.
We then, in a second step, effectively shrink all the raw $\hat{\tau}_j$ towards the overall estimate of $\hat{\tau}$ to get $\tilde{\tau}_j$, shrinking more the lower our estimate of $\omega$ is.
These $\tilde{\tau}_j$ are the Empirical Bayes estimates for the individual site impacts; while biased, they are known to be better point estimates (in terms of mean-squared error) for the full collection of true site-level impacts than the raw estimates $\hat{\tau}_j$.

Under a parametric or empirical Bayes framework, we can also obtain standard error estimates $\tilde{SE}_j$ for our $\tilde{\tau}_j$.
Using the estimate and standard error, we can finally conduct hypothesis testing as before, taking the test statistic of $t = \tilde{\tau}_j / \tilde{SE}_j$ as standard normal statistic and calculating a $p$-value as usual.
As discussed in the introduction, however, confidence intervals constructed using these standard errors do not satisfy frequentist coverage criteria, where, e.g., for a fixed $\tau_j$ the interval $(\tilde{\tau}_j \mp 1.96 \tilde{SE}_j)$ would cover $\tau_j$ 95\% of the time over repeated samples of the data.
Instead, they satisfy EB coverage, where appropriate coverage holds over repeated samples of the data \textit{and} $\tau_j$ from the data-generating Normal model.
In other words, hypothesis tests run in this way will not have the usual guarantees on Type-I error rate control for the site-specific effects.
That being said, they may perform decently well in practice.

Without shrinkage, power is simply a function of the un-shrunk standard error $SE_j$ and the true average effect $\tau_j$ for site $j$.
Given the shrinkage, the power to detect an individual site effect will additionally depend on the general size and distribution of average effects at other sites.
Of course, the most relevant parameter will be the site's average effect, but the distribution of impacts estimated at the other sites will affect how each site estimate is shrunk towards the overall average impact.
The question is how much this shrinkage helps or hurts the power and validity of the testing procedure, especially for those sites that actually have impacts far from the overall average.


\section{A simulation-based power calculator}

To calculate power for a multisite trial, we need to specify parameters for both the individual site of interest and its context.
While a typical power analysis for a single site only requires specification of a single hypothesized effect size (along with significance and power requirements), in a multisite trial the site's context, i.e., the number, size, and distribution of other sites, will also affect the effect estimate for that site.
The question is then: for a given site of interest, within a given context of interest, what is the chance of rejecting the null of no effect for that site?

We answer this question via simulation.
We repeatedly generate our target site and an associated context, analyze the synthetic data, and record whether we have detected an effect.
The frequency with which we reject the null across our simulations is then the power for that site and context.
We propose three general tools for conducting such a simulation.

\paragraph{Set-site simulation.} A straightforward way to conduct a multisite power simulation is to specify the site, and then to separately specify the sites in its context.
This completely decouples the site from its context.
For example, we could set the value of the true effect for site 1 to be $\tau_1 = 7$, and then sample the true effects for the $J-1$ other sites as $\tau_2, \dots, \tau_J \sim N(\tau, \omega)$.
For each set value of $\tau_1$, we could repeatedly generate data from a context to determine the power for that site/context combination.

This simulation strategy has two unusual features.
First, the full collection of sites 1 through $J$ no longer matches any data-generating model, because the site of interest is manually specified, unlike its context, which is still randomly generated according to particular parameters.
Second, for a fixed context, changing the site effect of interest naturally changes features of the full dataset on which the model is fit.
For example, if we fix the context as $\tau_2, \dots, \tau_J \sim N(\tau, \omega)$, setting $\tau_1 = -10$ versus $\tau_1 = 10$ will change $\bar{\tau} = \frac{1}{J} \sum_{j=1}^{J} \tau_j$, so even though the context remains the same, the direction in which the model shrinks its effect estimates can change.
We therefore need to be careful when interpreting comparisons across different $\tau_0$ values given a particular context.
That being said, the set site approach provides a clean answer to the question: if my site of interest is as specified, and it is otherwise within a particular context, what is the chance that I detect an effect for it?

\paragraph{Remove-site simulation.} Another way to conduct a multisite power simulation is to first specify and simulate all $J$ sites, and to then choose a particular site as the ``site of interest'' and remove it from its ``context,'' which would consist of the remaining $J-1$ sites.
This can be done for each of the generated sites in every simulation.
If we repeatedly do this, we can get power curves by grouping together sites that happen to have the same true impacts and seeing how often we reject the null for them.\footnote{Of course, given the continuous nature of the random site effects $\tau_j$, sites will never have the exact same true effect.
	We therefore round the randomly generated $\tau_j$ to the nearest 0.05 before generating the individual responses within sites.
	Alternatively, we could, e.g., repeatedly simulate data from a context where $\tau_j \sim N(0, 1)$, and summarize the power for all sites for which the true $\tau_j \in [0.95, 1.05]$ to estimate  power for sites with $\tau_j$ ``near'' 1, repeating this for $\tau_j$ values ``near'' as many other values as we'd like.}

This approach, however, has an undesirable feature.
Suppose we want to know the power for a site with $\tau_j = 1$ in the context $N(\tau, \omega)$.
When computing this power, we would systematically exclude all sites with $\tau_j = 1$ from the context; in other words, our true context would be $N(\tau, \omega \mid \tau_j \neq 1)$, the usual context with a gap at the specified $\tau_j$ value.
Besides making things difficult to interpret, this phenomenon also makes it hard to compare results across different $\tau_j$ values, since the context always changes.
As such, we opt to not employ this simulation strategy.

\paragraph{All-site simulation} One final way to conduct a multisite power simulation is to use remove-site simulation, but to treat the full collection of $J$ sites as the ``context'' for each site of interest instead of only considering the $J-1$ other sites as the context.
This approach circumvents the previous complications regarding the data-generating distribution; data are generated exactly according to the theoretical model, and there are no gaps or additions caused by removing or adding $\tau_j$ values for the site of interest.

All-site simulation, however, still runs into similar complications as set-site simulation.
Because each site of interest is included within its ``context,'' sites of interest with higher $\tau_j$ values will naturally have contexts with higher $\bar{\tau}$ values, on average.
As with set-site simulation, we need to be careful when interpreting comparisons across $\tau_j$ values for a fixed context.
%For a given context $G(\cdot)$, there is natural variation between simulations of $\tau_1, \dots, \tau_J \sim G(\cdot)$; in particular, some simulations will have higher true $\bar{\tau} = \frac{1}{J} \sum_{j=1}^J \tau_j$ values and others will have lower $\bar{\tau}$ values, simply due to random chance, especially if $J$ is small.
%When we summarize the results for higher values of $\tau_j$ across simulation runs of the same theoretical context $G(\cdot)$, we notice that those higher-$\tau_j$-value sites come from simulated contexts with higher $\bar{\tau}$ values, on average.
%In other words, because we include the sites of interest within their ``contexts,'' sites of interest with higher $\tau_j$ values will naturally have contexts with higher $\bar{\tau}$ values.
%Given a fixed context-generating distribution $G(\cdot)$, this makes comparison across different $\tau_j$ values for the site of interest quite challenging, because the higher $\tau_j$ values come from simulations with higher $\bar{\tau}$ values.
Another minor challenge with the all-site approach is that we cannot directly specify the $\tau_j$ values of our sites of interest, which means that the range of possible $\tau_j$ values (and how frequently extreme $\tau_j$ values appear) is constrained by our context.

As a result, all-site simulation answers a slightly different question than does set-site simulation: if I have a particular context and am interested in a site that happens to have a particular true effect within that context, what is the chance that I detect an effect for it?
This question allows for the fact that, e.g., having a large $\tau_j$ value for the site of interest will affect the model's estimates, even if we control for the context.

\paragraph{Summary of possible strategies} The three possible strategies for generating full collection of true site effects are summarized below:
\begin{itemize}
	\item Set $\tau_1$ to a given value, and generate $\tau_2, \dots, \tau_J \sim G(\cdot)$: set-site simulation
	\item Generate $\tau_1, \dots, \tau_J \sim G(\cdot)$, and then:
	\begin{itemize}
		\item For each site $j$, treat the $J-1$ other sites as the context: remove-site simulation
		\item For each site $j$, treat the full collection of $J$ sites as the context: all-site simulation
	\end{itemize}
\end{itemize}

For now, we have generated some results under all-site simulation.
If desired, we can reproduce the results under set-site simulation, if that feels easier to interpret.


\section{Simulation}
To understand how multilevel modeling potentially helps power and also undermines control of error rates, we conducted a multifactor simulation across a range of scenarios.

\paragraph{Conducting the simulation.}
To generate multisite data, we use the \texttt{blkvar} package in R; this package provides a host of methods for generating multisite data and analyzing it with a variety of models.
We first generate our collection of sites, then round site impacts to the nearest 0.05, and then generate the individuals within the site.
We finally take the generated data and analyze it with whatever planned analytic approach we have decided on.

%The parameters we vary are:
%\begin{itemize}
%	\item $\bar{n}$: the average number of students per site
%	\item $J$: the total number of sites
%	\item $ICC$: the variance of the site intercepts
%	\item $\tau$: the overall average treatment effect
%\end{itemize}
In our simulations, we use: $\bar{n} = 25, 50, 75, 100$; $ICC = 0, 0.3, 0.6, 0.9$; and $\tau = 0.01, 0.2, 0.5, 0.8$.
We fix $J = 20$ and assume that the treatment proportion in each site is $p=0.5$.
\cmntC{TODO: add $\omega$ treatment variation factors of 0, 0.3, 0.6}

We compare two methods: the individual site testing (where we ignore the contextual data for a given site and just use data for that site), and the multilevel modeling, Empirical Bayes estimation process.
We use the all site simulation approach, so each site for each simulation draw gets binned and used to calculate power for sites of that impact size, so each simulated multisite trial involves $J$ tests for $H_0: \tau_j \leq 0$.
All tests are one-sided and at $\alpha=0.10$ to maximize overall power (we assume in this context a researcher would be more liberal with their testing).


\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{power_plot_comp}
	\caption{Plot of power (at $\alpha = 0.1$) vs. true site ATE}
	\label{fig:power_plot}
\end{figure}


\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{power_plot_comp_ATE02}
	\caption{Plot of power (at $\alpha = 0.1$) vs. overall average ATE ($\tau$) for sites with $\tau_j = 0.20$}
	\label{fig:power_plot_ATE02}
\end{figure}


\subsection{Results}


Primary results are shown in Figure~\ref{fig:power_plot} and Figure~\ref{fig:power_plot_ATE02}.
The first figure shows power curves as a function of the true average treatment effect for an individual site.
The second figure shows power curves for a site with a true $\tau_j = 0.20$, as a function of the true overall average treatment effect $\tau$.
The solid lines are the MLM models, and the dashed the individual site.

The main trends are as expected: as Figure~\ref{fig:power_plot} shows, power increases as the informativeness of the site data increases, i.e., as $\bar{n}$ or $ICC$ increase, the site-specific standard error (shrunk or otherwise) goes down and power goes up.
We next discuss the power of the MLM models, and then compare that power to the simple-difference models.


\subsubsection{MLM power analyses}

Figure \ref{fig:power_plot} shows the power under various simulation settings, plotted against the true ATE for the site.
As mentioned before, we round the simulated true ATE values $\tau_j$ to the nearest 0.05 before generating student data, so the x-axis in Figure \ref{fig:power_plot} uses the rounded $\tau_j$ values.
The horizontal dashed reference lines are at $\alpha = 0.1$ and $0.80$; ideally, the power curve would approximately follow a step function along the dashed lines, with a rejection rate less than 0.1 for true ATE values less than 0, and a rejection rate over 0.8 for true ATE values greater than 0.

We see that as $\bar{n}$ and $ICC$ increase, the power curve approaches the desired step function, as expected.
We also note that MLM shrinkage has a strong effect when information is low (i.e., $\bar{n}$ and/or $ICC$ are low): for high values of $\tau$, the $\tau_j$ estimates are pulled upward, which increases power across all $\tau_j$ values.
This increase in power, however, also results in an elevated type-I error rate for the sites with $\tau_j \leq 0$.


In Figure \ref{fig:power_plot_ATE02}, we focus on only those sites for which $\tau_j = 0.2$, i.e., sites with a moderate effect size that we would hope to be able to detect.
We note that only when there is little within-site variation ($ICC$ is high) and/or large sample size ($\bar{n}$ is high) do we achieve a power of 0.8.
Also, when the overall site average impact ($\tau$) is high, the shrinkage does help as shown by the upward slant of the lines.

To more fully examine shrinkage for the $\tau_j = 0.20$ sites, see Figure \ref{fig:power_plot_ATE02_dens}.
This figure shows the density curves of the estimated $\hat{\tau}_j$ values across our simulation settings.
We can see the shift in the distributions to the right in the low-information (low $ICC$, low $\bar{n}$) settings, but not in the high-information settings.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{power_plot_ATE02_dens}
	\caption{Distribution of $\hat{\tau}_j$ for sites with $\tau_j = 0.20$ as a function of overall average impact, ICC, and site size}
	\label{fig:power_plot_ATE02_dens}
\end{figure}


\subsubsection{Comparing MLMs with single-site estimates}

We next compare the power of MLM estimates to the power of single-site estimates, i.e., the treatment-effect estimates we obtain when we ignore the data from the other sites.
We run single-site estimates using a standard Welch's $t$-test with pooled variances.

These single-site estimates are the dashed lines on Figures~\ref{fig:power_plot} and \ref{fig:power_plot_ATE02}.
First we see that MLMs reject either as or more frequently than their single-site counterparts, in all simulation settings tried.
The biggest difference in rejection rates occurs when $\bar{n}$ and/or $ICC$ are low.
In these low-information settings, MLMs have inflated type-I error rates for high values of $\tau$ (i.e., for contexts where the true average impact across sites is large).
When the overall site average is low, however, they have reasonable type-I error rates for when $\tau_j \leq 0$ but still maintain increased power for those sites with larger values of $\tau_j$.


\cmntC{Q: how to interpret? Low $\bar{n}$ means high standard errors. $ICC=0$ means all variation is residual, and not in site intercepts, i.e., higher standard errors.
	This means that we can pool across sites super well, and do better?
	This means that RIRC model has singular intercept fit (probably), and...}

\cmntM{This is an odd result.  Is it due to the high site increasing the average ATE so we are cherry picking samples with overall high sites?  We should do the set site analysis to check, possibly.}

\cmntC{Idea: when $\tau_j$ is higher, it's more likely to come from a simulation where $\bar{\tau}$ is higher, which means upward shrinkage and higher power! This is super annoying. Perhaps doing `set site simulation' will be necessary...}

\cmntC{I ran a quick check, and higher $\tau_j$ values do indeed tend to come from simulations with higher averages of all of the $\tau_j$ values...}

The dashed horizontal lines on Figure~\ref{fig:power_plot_ATE02} for the $\tau_j = 0.20$ sites show a similar pattern, with the divergence of power increasing as the true average increases, and the divergence increasing more quickly in the low information settings.

Figure~\ref{fig:power_plot_comp_ATE02_dens} compares the single-site estimates and MLM estimates for a $\tau_j=0.20$ site across our $ICC = 0$ scenarios.
This figure shows the impact of shrinkage: the estimated impacts are less spread out and generally closer to the vertical red line denoting 0.20 (the true effect for the site).
As $\tau$ increases, we also see the MLM estimates are shifted toward the right, as expected with shrinkage.
Overall, the MLM model is improving the estimation in terms of how far off the estimate is from the truth.

\cmntM{Perhaps a plot with the RMSE to make this point more forcefully?  In this case, also calculate the RMSE of using the simple estimate average impact for all sites for the site of interest.}

\cmntC{RMSE for sites with $\tau_j = 0.2$ is indeed better using the MLMs, more so in the low-information setting.}

\subsubsection{Discussion}
Overall, MLM shrinkage increases rejection rates, relative to our baseline, when $\tau$ is high and site information is low, which increases both the power and the type-I error rate (type-I error is the rejection rate when the true site impact ($x$-axis) is 0.
For the other settings, however, using a multilevel model does not seem to improve power for one-tailed tests for single sites, when compared to simply running a $t$-test on that site.
In most cases, power is low: MLM is not allowing for any reasonable level of rigor in investigating individual site effects.
That being said, if one really does want a decent guess as to an individual impact, MLM is a far better option, in this context, than the simple difference.


\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{power_plot_comp_ATE02_dens}
	\caption{Plot of power (at $\alpha = 0.1$) vs. true site ATE}
	\label{fig:power_plot_comp_ATE02_dens}
\end{figure}

\appendix
\section{Appendix A: }

TODOs:
\begin{itemize}
	\item Singular fit rate analysis
	\item Show that rounding $\tau_j$ isn't a horrible sin
\end{itemize}


\cmntM{ Further note: We should perhaps re-do all this with $\tau_j = 0.40$ for a ``big'' effect size, rather than what we might hope is a median site.   This big site would be about 1SD above average in a context with a modest effect and moderate cross site variation.}



	
\end{document}