\documentclass[]{article}

% math packages
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}

% for coloring in a table
%\usepackage[table,xcdraw]{xcolor}

% for subfigures
\usepackage{caption}
\usepackage{subcaption}

% including graphics
\usepackage{graphicx}
\graphicspath{ {./images/} }

% drawing graphs
\usepackage{tikz-cd}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!10]
\tikzstyle{arrow} = [thick,->,>=stealth]

% hyperlinks
\usepackage{hyperref}

% some useful shortcuts
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\indep}{\perp\!\!\!\!\perp}
\newcommand{\blambda}{{\bm{\lambda}}}
\newcommand{\btheta}{{\bm{\theta}}}
\newcommand{\bpsi}{{\bm{\psi}}}

\newcommand{\by}{\mathbf{y}}

\usepackage{setspace}
\doublespacing

\usepackage{natbib}
\bibliographystyle{rusnat}

% Editing macros
\usepackage{color}
\newcommand\cmnt[2]{\qquad{{\color{red} \em #1---#2} \qquad}}
\newcommand\cmntM[1]{\cmnt{#1}{Miratrix}}
\newcommand\cmntC[1]{\cmnt{#1}{Che}}
\newcommand\awk{{{\color{red} {$\leftarrow$ Awkward phrasing}}\qquad}}
\newcommand\cmntMp[1]{{\color{red} $\leftarrow$ {\em #1 -Miratrix} \qquad}}

\usepackage{natbib}
\bibliographystyle{rusnat}

%opening
% \title{On power analyses for individual site impacts in multisite trials}
\title{On power analyses for site-level treatment effects in multisite trials}
\author{Jonathan Che \& Luke Miratrix}

\begin{document}

\maketitle

% NOTE: "site effect" is confusing terminology. Let's use:
%  - overall average treatment effect  // overall impact?
%  - site-level treatment effect  // site impacts?

\section{Introduction}

Researchers typically power multisite trials to estimate whether their intervention of interest improves outcomes on average, across all sites.
They can use existing tools to design multisite trials that have high power for this overall average treatment effect [add citation, PUMP or something else?].
If desired, researchers can also power multisite trials to detect given levels of cross-site impact variation [add citation].

In many applications, however, it may also be important to powerfully estimate the treatment effects within individual sites.
Site stakeholders are often much more interested in whether the treatment improved outcomes within their individual site than in whether the treatment appeared to be effective overall.
Estimation of site-level treatment effects becomes particularly important when researchers suspect substantial unmodeled heterogeneity across sites, which occurs in many settings.

This paper provides guidelines for researchers interested in designing multisite trials to effectively estimate individual site-level treatment effects.
We suggest that to achieve this goal, researchers should focus on characterizing the average margins of error from the interval estimates produced by their estimation method of choice, for their design of choice.
We support our suggestion via a careful study of the site-level interval estimates produced by standard methods for analyzing multisite trials, clarifying common points of confusion regarding their interpretation.
We then use a detailed simulation study to illustrate how design factors affect average margin of error.

The paper proceeds as follows.
Section 2 provides background on site-level treatment-effect estimation in multisite trials.
In Section 3 we introduce our simulation framework and modeling methods.
Section 4 steps through an illustrative example that motivates the suggestions we make in this paper, and Section 5 elaborates via an extensive simulation study.
Finally, in Section 6 we provide a useful template for a power analysis conducted under our proposed framework, based on a real-world example.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background}

\subsection{Mathematical framework}

In a multisite trial, the same randomized trial is conducted within each of several sites.
Rather than separately estimating the treatment effect for each site, it is well known that shrinking the noisy site-level treatment-effect estimates toward an overall average produces a better collection of estimates in terms of mean-squared error \citep{james1961estimation}.
For $J$ total sites, this shrinkage can be motivated by the following mathematical framework:
\begin{align*}
    \hat{\tau}_j &\sim N(\tau_j, \text{s.e}_j), \ \ j=1,\dots,J \\
    % \tau_j &\stackrel{i.i.d.}{\sim} G.
    \tau_j &\sim G.
\end{align*}
Under this framework, site-level treatment-effect estimates $\hat{\tau}_j$ are noisy realizations of true site-level treatment effects $\tau_j$, with observed standard errors $\text{s.e}_j$.\footnote{If the standard errors $\text{s.e}_j$ are estimated using only data from site $j$, the multisite trial can be viewed as a ``planned meta-analysis.''
In this paper, we simply assume that the $\text{s.e}_j$ values are fixed, so they can be computed using data from all sites.
This accommodates a wide range of popular models for analyzing multisite trials, e.g., the fixed-intercepts, random coefficients model proposed in \cite{bloom2017using}.}
We then assume that the true site-level treatment effects $\tau_j$ themselves come from some distribution $G$.
The assumed form of this distribution $G$ determines the level of shrinkage that should be applied to the site-level treatment-effect estimates.

\subsection{Estimation methods}

There are many different methods for producing point and interval estimates of site-level treatment effects.
These methods can generally be categorized using two factors: whether they are parametric or nonparametric, and whether they are empirical Bayes or fully Bayesian.
% The many methods for estimating site-level treatment effects under the framework above can generally be categorized using two factors: whether they are parametric or nonparametric, and whether they are empirical Bayes or fully Bayesian.

Applied researchers typically use parametric methods to analyze multisite trials.
Empirical Bayes random-effects and mixed-effects models are particularly popular due to their ease of implementation \citep{bloom2017using}, though Bayesian methods have been used in many applications as well \citep{rubin1981estimation}.
The literature on random-effects meta-analysis \citep{higgins2009re} and empirical Bayes confidence interval methods \citep{morris1983parametric, he1992parametric, gene2009empirical} also contains many parametric procedures for problems similar to the analysis of multisite trials.
Standard parametric methods assume $G \sim N(\mu, \sigma^2)$ for unknown mean and variance parameters $\mu$ and $\sigma^2$, which must then be estimated.
Empirical Bayes methods typically use maximum likelihood and/or restricted maximum likelihood approaches to estimate $\mu$ and $\sigma$, while fully Bayesian methods set parametric priors on $\mu$ and $\sigma$ and either analytically derive or use Monte Carlo Markov chain (MCMC) sampling to estimate their posterior distributions.

In recent years, semi- and non-parametric methods for this problem have received significant attention in the theoretical literature.
These methods avoid placing strong parametric assumptions on the distribution of $G$ while maintaining the statistical properties and computational feasibility of parametric methods.
Semiparametric empirical Bayes procedures flexibly estimate $G$ from the data \citep{laird1987empirical, yu2018adaptive}, while nonparametric empirical Bayes methods assume a general class of distributions for $G$ \citep{ignatiadis2022confidence} or avoid distributional assumptions on $G$ entirely \citep{armstrong2020robust}.
Nonparametric Bayesian methods place highly flexible priors on $G$, such as Dirichlet process priors (e.g., cite some DP stuff).

In this paper, we focus on parametric empirical Bayes and Bayesian methods for analyzing multisite trials, since they are much more frequently used than their nonparametric counterparts.
While parametric models are relatively simple, advice for conducting power analyses on them remains unclear, particularly for the estimation of individual site-level treatment effects.
We aim to clarify the use of parametric models for this task and leave the evaluation of nonparametric methods to future work.

\subsection{Motivation}

To conduct power analyses for the site-level treatment effects $\tau_j$ in multisite trials, we must first define coverage for the interval estimates of the $\tau_j$ values.
In a single-site randomized trial, standard interval estimates cover the site's true treatment effect $100(1-\alpha)$\% of the time for a given significance level $\alpha$, across (hypothetical) repeated trials.
Because of shrinkage, the interval estimates produced by empirical Bayes and Bayesian procedures generally do not have this property for the site-level treatment effects $\tau_j$ in multisite trials.\footnote{Some interval estimates, such as those proposed by \citet{yu2018adaptive}, satisfy coverage for each $\tau_j$, but these types of interval estimates are not common in the literature.}
Instead, these intervals typically possess a so-called empirical Bayes coverage property \citep{morris1983parametric}, which guarantees $100(1-\alpha)$\% coverage across the collection of $\tau_j$ values for all $J$ sites, rather than $100(1-\alpha)$\% coverage for each individual site.
In particular, shrinkage induces the intervals generated by these procedures to overcover for values of $\tau_j$ close to the overall mean and undercover for values far from it \citep{snijders2011multilevel}.

While this behavior is well-understood in a statistical sense, it can lead to practical confusion for site stakeholders and research analysts alike.
For example, what should a site stakeholder expect from the final interval estimate for their particular site?
Is the notion of power induced by the definition of empirical Bayes coverage useful for the research analyst who wants to design multisite trials interested in estimating site-level treatment effects?
In the sections that follow, we use an illustrative example along with a simulation study to clarify the statistical properties of empirical Bayes and Bayesian interval estimates for multisite trials, which in turn helps to answer these questions and provide a practical path forward.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Methods}

\subsection{Simulation setup}

For our simulation studies, we simulate data from a multisite trial and estimate the site-level treatment effects $\tau_j$.
We simulate multisite trial data using the \texttt{blkvar} package in R [CITE].
Specifically, we sample observed outcomes $Y_{ij}$ for student $i$ in site $j$ as:
\begin{align*}
	Y_{ij} &= \alpha_j + \tau_j Z_{ij} + \epsilon_{ij} \\
	\alpha_j &= \alpha + u_{0j} \\
	\tau_j &= \tau + u_{1j} \\
	\begin{pmatrix}
		u_{0j} \\ u_{1j}
	\end{pmatrix} &\sim N\left(
	\begin{pmatrix}
		0 \\ 0
	\end{pmatrix}, 
	\begin{bmatrix}
		\sigma^2_\alpha & \rho_{01} \\  & \sigma^2_\tau
	\end{bmatrix}\right) \\
	\epsilon_{ij} &\sim N(0, \sigma^2_y) ,
\end{align*}
where the $\alpha_j$ are random site-level intercepts and the $\tau_j$ are the true site-level treatment effects.
We conduct the simulation in effect-size units, so $Var(Y_{ij} \mid Z_{ij}=0)=1$, i.e., $\sigma^2_\alpha + \sigma^2_y = 1$.
We also set $\rho_{10}=0$ so the intercept and treatment random effects are uncorrelated.

For simplicity, we condense the data within each site to an average treatment effect $\hat{\tau}_j$ and an estimated standard error $\text{s.e}_j$ before applying shrinkage estimators.
We let $\hat{\tau}_j$ be the simple difference-in-means estimate, $\hat{\tau}_j = \frac{1}{n_{1j}} \sum_i Z_{ij}Y_{ij} - \frac{1}{n_{0j}} \sum_i (1-Z_{ij})Y_{ij}$, where $n_{1j}$ and $n_{0j}$ represent the number of treated and control units within site $j$, respectively.
The estimate of $\text{s.e}_j$ varies with the analytic method; meta-analytic approaches are restricted to using standard errors separately estimated within each site, while other methods can pool information across sites to estimate these standard errors.
Our condensed data ultimately takes the form:
\begin{align*}
    \hat{\tau}_j &\sim N(\tau_j, \text{s.e}_j), \ \ j=1,\dots,J \\
    \tau_j &\sim N(\tau, \sigma_\tau^2).
\end{align*}

\subsection{Analytic methods}

We apply two different methods to produce interval estimates for each site-level treatment effect.
For a baseline comparison, we record 90\% confidence intervals on $\tau_j$ from single-site t-tests, conducted separately at each site (``Single'').
We also record the 90\% credible intervals on $\tau_j$ from a fully Bayesian multilevel model (``MLM''), with $G \sim N(\tau, \sigma^2)$, priors $\tau \sim N(0,5^2)$ and $\sigma \sim N(0,1)$, and treatment-condition-pooled site-level standard error estimates \citep{bloom2017using}.\footnote{Specifically, we let $\text{s.e}_j = \frac{\sigma_1^2}{n_{1j}} + \frac{\sigma_0^2}{n_{0j}}$, where $\sigma_{1}$ and $\sigma_{0}$ are estimates of the observation-level outcome variance for treated and control units, respectively, pooled across all sites.}

We choose to use a fully Bayesian model for two reasons.
First, Bayesian models provide a standard framework for model comparison, since many parametric empirical Bayes approaches are equivalent to Bayesian models with particular priors.
More importantly, though, Bayesian models naturally estimate intervals for the $\tau_j$ parameters.
Standard parametric empirical Bayes procedures estimate intervals for the overall $\tau$ and $\sigma^2$ parameters, but they typically only produce point estimates of the $\tau_j$ parameters.
While analysts can generate interval estimates for the $\tau_j$ parameters (e.g., via the bootstrap), it is not common practice and standard software packages do not include this functionality.

We note that the MLM we use is well-specified under our simulation setup, so the simulation results generally represent best-case results.
While it is well-known that parametric normal models can produce suboptimal results when $G$ is non-normal \citep{armstrong2020robust}, the normal model is decently robust in most applied settings [CITE].
In any case, our simulation study focuses on studying the behavior of shrinkage estimators.
Though misspecification affects the performance of these estimators, it does not strongly affect their general behavior, so we proceed with the simple well-specified model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Illustrative example}

\subsection{Single simulation}

We highlight the main ideas in this paper using a single simulated dataset.
For 100 simulated $\tau_j$ values, Figure \ref{fig:shrinkageplot} shows the 90\% interval estimates produced by the Bayesian multilevel model and single-site t-tests.
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{shrinkageplot}
	\caption{Interval estimates for one simulated dataset. Intervals overlapping the dotted line cover their respective true $\tau_j$ values.}
	\label{fig:shrinkageplot}
\end{figure}
In this simulation, 90 of the MLM intervals and 91 of the single-site intervals contain their respective true $\tau_j$ values.

The two methods, however, achieve this approximate 90\% coverage in different ways.
As seen in Figure \ref{fig:shrinkageplot}, the MLM shrinks its point estimates toward the overall average.
This shrinkage improves coverage for moderate $\tau_j$ values, but causes the MLM to overestimate the lowest $\tau_j$ values and underestimate the highest $\tau_j$ values.
For example, Figure \ref{fig:shrinkageplot_slice1} highlights the intervals for sites with true $\tau_j \approx 0.6$.
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{shrinkageplot_slice1}
	\caption{Interval estimates for one simulated dataset. Intervals for $\tau_j \approx 0.6$} are highlighted.
	\label{fig:shrinkageplot_slice1}
\end{figure}
While the single-site intervals are randomly scattered around the dotted line, shrinkage pulls the MLM intervals downward, causing many of them to fail to cover.

On the other hand, shrinkage ensures that when an MLM estimates that a site has a relatively extreme value of $\tau_j$, the site's true $\tau_j$ value is indeed likely to be extreme.
Figure \ref{fig:shrinkageplot_slice3} replicates Figure \ref{fig:shrinkageplot_slice1}, now highlighting intervals for sites with large estimated site-level treatment effects ($\hat{\tau}_j \approx 0.55$).
\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{shrinkageplot_slice3}
	\caption{Interval estimates for one simulated dataset. Intervals for $\hat{\tau}_j \approx 0.55$ are highlighted.}
	\label{fig:shrinkageplot_slice3}
\end{figure}
We notice that the large MLM estimates are more clustered toward the right-hand side of the plot than are the large single-site estimates.
In other words, the large MLM estimates are more likely to correspond to truly large values of $\tau_j$.

This example reviews how to properly interpret the interval estimates produced by MLMs.
While specific coverage properties depend on model specification \citep{armstrong2020robust}, MLM intervals will generally contain their respective true parameters at close to the correct rate.
Given observed data, each interval estimate has a high probability of containing its respective $\tau_j$ value.
MLM shrinkage means, however, that extreme $\tau_j$ values will categorically be difficult to correctly estimate.

To more precisely quantify these ideas of coverage conditional on $\tau_j$ or $\hat{\tau}_j$ value, we simulate 1000 multisite trial datasets and summarize the results in the figures below.
In a traditional power analysis, the analyst posits an effect size $c$ and computes power for that effect size.
An analogous procedure for the site-level treatment effects in a multisite trial is to compute power for each true site-level treatment effect size $\tau_j$.
Figure \ref{fig:pcp1} visualizes the results of a power simulation under this setup.\footnote{For clarity, we round the true site-level treatment effects $\tau_j$ to the nearest 0.05 prior to simulating site-level data, and we only visualize the power curves for $\tau_j =$ 0, 0.2, and 0.4.}
\begin{figure}[ht]
    \centering
    \begin{subfigure}[a]{\textwidth}
    	\includegraphics[width=\textwidth]{pp1}
        \caption{Rejection rate}
    	\label{fig:pp1}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{\textwidth}
    	\includegraphics[width=\textwidth]{cp1}
        \caption{Coverage}
    	\label{fig:cp1}
    \end{subfigure}
    \caption{Power and coverage for $\tau_j = 0, 0.2, 0.4$ at $\alpha=0.1$, from 1000 simulations ($J=25$, ICC=0.2, $\tau=0.2$, $\sigma=0.2$).}
    \label{fig:pcp1}
\end{figure}
Figure \ref{fig:pp1} plots the average rejection rate against the average site size for sites with $\tau_j =$ 0, 0.2, and 0.4.
Clearly, MLMs improve power relative to the single-site interal estimates.
Unfortunately, we also notice that when $\tau_j = 0$, shrinkage causes MLMs to make false rejections at a rate higher than $\alpha=0.1$.
As previously noted, this is due to the fact that the interval estimates produced by MLMs undercover extreme values of $\tau_j$ (Figure \ref{fig:cp1}).

Figure \ref{fig:pp2} illustrates power conditional on \textit{estimated} site-level treatment effects $\hat{\tau}_j$.
\begin{figure}[ht]
    \centering
    \begin{subfigure}[a]{\textwidth}
    	\includegraphics[width=\textwidth]{pp2}
        \caption{Rejection rate}
    	\label{fig:pp2}
    \end{subfigure}
    \vfill
    \begin{subfigure}[b]{\textwidth}
    	\includegraphics[width=\textwidth]{cp2}
        \caption{Coverage}
    	\label{fig:cp2}
    \end{subfigure}
    \caption{Power and coverage for $\hat{\tau}_j = 0, 0.2, 0.4$ at $\alpha=0.1$, from 1000 simulations ($J=25$, ICC=0.2, $\tau=0.2$, $\sigma=0.2$).}
    \label{fig:pcp2}
\end{figure}
We again notice that MLM intervals have better power than the single-site intervals.
Importantly, Figure \ref{fig:cp2}) shows that this increased power does not come at the cost of invalid tests; instead, we notice that while MLMs are approximately valid, the single-site intervals undercover, particularly for extreme estimated $\hat{\tau}_j$ values.
As shown in the illustrative example, the small sample sizes within each site cause single-site intervals to be noisy; as a result, they often estimate extreme $\hat{\tau}_j$ values for sites that do not have extreme true $\tau_j$ values, resulting in poor coverage across the collection of extreme interval estimates.
MLMs use shrinkage to avoid this problem.

\subsection{Initial conclusions}

Traditional power analyses help researchers compute the sample size needed to achieve 80\% power for a range of potential effect sizes.
As the example above demonstrates, however, this concept of ``potential effect sizes'' becomes unwieldy in multisite trials.
The empirical Bayes coverage targeted by many shrinkage methods targets average coverage across a distribution of effect sizes.
As a result, multilevel model intervals are invalid for particular true effect sizes $\tau_j$.
Without shrinkage, though, naive t-test intervals are too noisy to be reliable indicators of treatment effects.\footnote{Note that \cite{yu2018adaptive} provides intervals that are better... These work with very small sample sizes...? (interrogate Evan again)}

To avoid these challenges, we suggest that researchers focus on characterizing the average length of the interval estimates produced by their method of choice, across the posited distribution of $\tau_j$ values.
Averaging any metric of choice across the distribution of $\tau_j$ achieves two goals.
On a basic level, doing so mirrors the concept behind empirical Bayes coverage, which is what most shrinkage methods aim to achieve.
More practically, averaging across $\tau_j$ values results in simpler prospective power analyses.
Prior to conducting a multisite trial, it is difficult to predict where a particular site's treatment effect $\tau_j$ may fall in the distribution $G$.
Since sites are exchangeable, however, analyzing the average performance of an interval estimator gives some indication of how that estimator will perform for each particular site, even without specifying each site's $\tau_j$ value.

We suggest using margin of error as the metric of choice to simplify interpretation.
We would ideally like to consider some form of average minimum detectable effect size (MDES), so that the multisite trial could be powered to consistently detect a particular effect size.
Unfortunately, it makes no sense to average MDES values across the distribution of $\tau_j$ values, since the MDES values themselves are the minimum $\tau_j$ values that achieve 80\% power.
Error margins, on the other hand, can be averaged across the distribution of $\tau_j$ values while still maintaining some of the primary motivations behind MDES.
Researchers can design a multisite trial to achieve some acceptable average margin of error for the site-level treatment effects.
Then, as a rule of thumb, the study would generally be powerful enough to detect effects larger than this average margin of error.

[NOTE: could do an interval length per $\tau_j$ value plot... essentially would show that $\tau_j=0.2$ has shorter intervals and extreme $\tau_j$ has wider intervals, not clear if this is helpful]

[One other motivation that I like, but it's not precise:
Average power begs the question: for which values of $\tau_j$ are the intervals more or less powerful?
This is a sad question, since conditioning on $\tau_j$ makes MLMs look bad.
Average interval length, on the other hand, begs the question: for which values of $\hat{\tau}_j$ are the intervals longer or shorter?
This is a happier question.]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Simulation study}

\subsection{Setup}

We conduct a simulation study to explore trends in how different data-generating factors affect average margin of error.
In particular, we let the number of sites $J = 10, 25, 100$; the overall average treatment effect $\tau = 0.1, 0.2, 0.3$; the cross-site effect heterogeneity $\sigma^2_\tau = 0.1, 0.2, 0.3$; and the intraclass correlation coefficient $ICC \equiv \frac{\sigma^2_\alpha}{\sigma^2_\alpha + \sigma^2_y} = 0.1, 0.2, 0.3$ to mirror standard data settings in education trials \citep{weiss2017much}.
To keep the simulation simple, we use the illustrative example simulation as our base case ($J=25$, $\bar{n}=5,10,\dots,200$, $\tau=\sigma_\tau=ICC=0.2$) and study the main effects of varying these factors one at a time.
For each set of data-generating parameters, we simulate 10,000 total sites.\footnote{We simulate 1000 datasets for $J=10$, 400 datasets for $J=25$, and 100 datasets for $J=100$.}

\subsection{Results}

Increasing the number of sites slightly improves average margin of error, particularly when sites are small.
Figure \ref{fig:results_J} plots the average margin of error for different values of $J$.
We see that, e.g., when $\bar{n}=25$, increasing the number of sites from 10 to 25 reduces the average margin of error by roughly 0.06 effect-size units, i.e., by about 15\%.
This reduction is roughly equivalent to the reduction associated with increasing $\bar{n}$ to 40 for the 10 sites.
In general, however, we note that the gains from adding more sites are overshadowed by the gains from adding more observations to each site;
if given a choice between the two, researchers interested in understanding site-level treatment effects should prioritize increasing sample sizes within study sites rather than adding more sites to their study.
% Power and coverage don't really change, so we omit those plots.
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{simstudy_J_length}
	\caption{Average margin of error across different values of $J$ ($\tau=0.2$, $\sigma_\tau=0.2$, $ICC=0.2$).}
	\label{fig:results_J}
\end{figure}

Low cross-site treatment-effect heterogeneity leads to significantly smaller average margins of error.
As shown in \ref{fig:results_txsd}, decreasing $\sigma_\tau$ values significantly improves average margins of error.
This indicates that our proposed power analyses are generally sensitive to the specification of $\sigma_\tau$;
incorrectly specifying low $\sigma_\tau$ values can lead to serious overoptimism about the average margin of error.
Researchers should carefully consider a range of reasonable values for $\sigma_\tau$ when powering their multisite trials to avoid this potential pitfall.
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{simstudy_txsd_length}
	\caption{Average margin of error across different values of $\sigma_\tau$ ($J=25$, $\tau=0.2$, $ICC=0.2$).}
	\label{fig:results_txsd}
\end{figure}
% Note: small $\sigma_\tau$ means large shrinkage, which means huge implications for interval interpretation

Varying $\tau$ and $ICC$ has little effect on average interval length in our simulations, so we omit these results.
The simulations sample $\tau_j \sim N(\tau, \sigma_\tau^2)$, so shifting $\tau$ naturally has no effect on average interval length.
If there is good reason to suspect that the site-level treatment effects do not come from a location family of distributions, however, it may become necessary to specify reasonable values of $\tau$ for a power analysis.
Increasing $ICC$ has very minimal effects on average interval length, since site-level intercepts do not affect estimated site-level treatment effects.\footnote{Our simulations are in effect-size units, i.e., $\sigma^2_\alpha + \sigma^2_y = 1$, so increasing $ICC$ decreases $\sigma^2_y$.
As a result, increasing $ICC$ in our simulations leads to very slightly smaller average margins of error.}

Overall, the simulations show that site-level sample size is the primary driver of average margin of error.
This result is intuitive; if site-level treatment effects are of interest, large site-level samples are required to precisely estimate them.
For the data settings we considered, approximately 50-100 observations were required on average per site in order to achieve a margin of error of around 0.25 at 90\% confidence.
If these sample sizes are unavailable, researchers may still be able to achieve reasonable average margins of error if there happens to be little cross-site treatment-effect heterogeneity.
Increasing the number of sites also improves average margins of error, but by far less than does increasing average site-level sample sizes.

% TODO: highlight importance of correct interpretation when there's a bunch of shrinkage?
% TODO: highlight absolute stuff. Like how hard it is to estimate precisely, even in our ``best'' data settings.

\section{Case study}

\subsection{Background: Scaling Up College Completion Efforts for Student Success (SUCCESS)}

% https://www.mdrc.org/project/scaling-college-completion-efforts-student-success-success#overview
To illustrate our suggested approach for powering multisite trials to detect site-level treatment effects, we conduct an example power analysis.
We base our case study on the Scaling Up College Completion Efforts for Student Success (SUCCESS) initiative designed by the Manpower Demonstration Research Corporation (MDRC).
SUCCESS aims to improve 3-year degree completion rates for traditionally underserved college students by implementing programs proven to work in these settings, such as ``frequent proactive advising; financial incentives tied to service usage; strategies focused on increasing academic momentum; and the use of real-time data to support student progress'' \citep{MDRCsuccess}.
Our prospective study will be conducted in ten community colleges across five states, with projected sample sizes known in advance.
Each school is interested in a principled estimate of the treatment effect at their site.
Scaling the program would likely involve state funding, so we are also interested in understanding treatment effects within each state.

\subsection{Data-generating model}

To conduct the power analysis, we simulate data under the following simplified model:
\begin{align*}
    Y_{ij} &\sim Bernoulli(p_{ij}) \\
    p_{ij} &= \alpha_j + Z_{ij} \tau_j \\
    \alpha_j &= \alpha + u_{0j} \\
	\tau_j &= \tau + u_{1j} \\
	\begin{pmatrix}
		u_{0j} \\ u_{1j}
	\end{pmatrix} &\sim N\left(
	\begin{pmatrix}
		0 \\ 0
	\end{pmatrix}, 
	\begin{bmatrix}
		\sigma^2_\alpha & \rho_{01} \\  & \sigma^2_\tau
	\end{bmatrix}\right).
\end{align*}
We truncate $p_{ij}$ to keep it within the interval (0,1).\footnote{We use this simple model rather than a logistic regression model for $p_{ij}$ for clarity, since it allows for direct translation of postulated effect sizes into the model.}
Past experimental evidence suggests that $\tau_j$ values are typically moderate, about 0-5 percentage points, though they can be as large as 18 percentage points.
With the onset of COVID-19, however, early treatment-effect estimates suggest more moderate effects, in the 2-4 percentage point range.
Base graduation rates are low, in the 15-20\% range.
We therefore simulate data using $\alpha = 0.175$, $\sigma_\alpha=0.01$, and $\tau=0.03$.
We let $\sigma_\tau \in \{0.1, 0.2, 0.3, 0.4, 0.5\}$ and $\rho_{01} \in \{0, 0.3, 0.6\}$.
Because each site/state's projected sample size is already fixed and known in advance, we fix these values and aim to infer how precisely we can expect to estimate their treatment effects under the posited distribution of treatment effects.

We also conduct a secondary simulation with a more realistic distribution for $(\alpha_j, \tau_j)$.
Given that the treatment program is elective, it seems unlikely that it could have a negative effect.
We also expect right skew in the distribution of site-level treatment effects, where it has minimal effect in most schools but a large effect in some schools.
To capture this skew, we simulate site-level treatment effects from a Gamma distribution:
\begin{align*}
    \alpha &\sim N(\alpha, \sigma^2_\alpha) \\
    \tau_j &\sim Gamma(a, b).
\end{align*}
We again use $\alpha = 0.175$ and $\sigma_\alpha=0.01$, but we fix the mean of the distribution of $\tau_j$ at 0.03 and vary its skew using the parameters $(1.5, 50)$, $(4.5, 150)$, and $(7.5, 250)$.
Figure \ref{fig:gammas} shows the resulting distributions of site-level treatment effects.
\begin{figure}
    \centering
    \includegraphics[width = \textwidth]{writeup/images/gamma.png}
    \caption{Gamma distributions considered for site-level treatment effects.}
    \label{fig:gammas}
\end{figure}

\subsection{Results}

We first show the main results for the college-level analysis.
Figure \ref{fig:case_study_moe} visualizes the average margins of error, plotted against the baseline single-site-estimate margins of error for comparison.
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{writeup/images/case_study_moe.png}
    \caption{Average margin of error in case study. Points correspond to prospective sample sizes at 10 community colleges. Dashed black line represents single-site-estimate margins of error.}
    \label{fig:case_study_moe}
\end{figure}
Naturally, the largest colleges enjoy the smallest average margins of error, though these margins unfortunately remain quite large, at roughly 4-5 percentage points.
The average margin of error is not sensitive to $\rho_{01}$, so we exclude those results, but it is very sensitive to the amount of cross-site treatment-effect heterogeneity.
For example, as $\sigma_\tau$ increases from 1 percentage point to 5 percentage points, the average margin of error for the smallest site increases by nearly 50\%.

The state-level analysis produces similar results.
Figure \ref{fig:case_study_states_moe}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{writeup/images/case_study_states_moe.png}
    \caption{Average margin of error in case study. Points correspond to prospective sample sizes within 5 states. Dashed black line represents single-site-estimate margins of error.}
    \label{fig:case_study_states_moe}
\end{figure}

Overall, the case study suggests that while it may be challenging to precisely estimate effects within each college, it may be more reasonable to do so for states.

\section{Conclusion}

In many settings, analysts may wish to power multisite trials to consistently detect effects at individual sites.
While this task appears straightforward at first glance, it is important to carefully consider how it interacts with the shrinkage models we use to estimate site-level treatment effects.
In this paper, we suggest that simulating the average interval length is the clearest, simplest method to power multisite trials for site-level treatment effects.



OTHER SIMULATION STUDY CONCLUSIONS:
Use larger sites, not more sites.

TO MOTIVATE OUR SUGGESTION:
We provide an illustrative example to clarify how shrinkage interacts with coverage.


\bibliography{refs.bib}
	
\end{document}